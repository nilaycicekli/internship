{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e7bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43fcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Contextual AI imports\n",
    "import xai\n",
    "from xai.explainer import ExplainerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ef402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on a subset of the 20newsgroups dataset (text classification)\n",
    "categories = [\n",
    "    'rec.sport.baseball',\n",
    "    'soc.religion.christian',\n",
    "    'sci.med'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b563e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# Fetch and preprocess data\n",
    "raw_train = datasets.fetch_20newsgroups(subset='train', categories=categories)\n",
    "raw_test = datasets.fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faa2da4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(raw_train.data)\n",
    "y_train = raw_train.target\n",
    "X_test = vectorizer.transform(raw_test.data)\n",
    "y_test = raw_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f425340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a model\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26149b",
   "metadata": {},
   "source": [
    "### Main Contextual AI steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9db7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the text explainer via the ExplainerFactory interface\n",
    "explainer = ExplainerFactory.get_explainer(domain=xai.DOMAIN.TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bcac216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the explainer\n",
    "def predict_fn(instance):\n",
    "    vec = vectorizer.transform(instance)\n",
    "    return clf.predict_proba(vec)\n",
    "\n",
    "explainer.build_explainer(predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "214437ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label soc.religion.christian => 2\n",
      "{0: {'explanation': [{'feature': 'taught', 'score': -0.0012630272627361225},\n",
      "                     {'feature': 'Heaven', 'score': -0.0013287439943891442},\n",
      "                     {'feature': 'must', 'score': -0.001375922792479016},\n",
      "                     {'feature': 'Scripture', 'score': -0.0014274476435107762},\n",
      "                     {'feature': 'Bible', 'score': -0.002310253356793093}],\n",
      "     'prediction': 6.798212345437472e-05},\n",
      " 1: {'explanation': [{'feature': 'Sin', 'score': -0.0029887528528975776},\n",
      "                     {'feature': 'Bigelow', 'score': -0.00298886560086397},\n",
      "                     {'feature': 'Heaven', 'score': -0.0035761247900124604},\n",
      "                     {'feature': 'Scripture', 'score': -0.003588574323814682},\n",
      "                     {'feature': 'Bible', 'score': -0.007372212218436996}],\n",
      "     'prediction': 0.00044272540371258136},\n",
      " 2: {'explanation': [{'feature': 'Bible', 'score': 0.009690019215086595},\n",
      "                     {'feature': 'Scripture', 'score': 0.005022211529046868},\n",
      "                     {'feature': 'Heaven', 'score': 0.004949814535434171},\n",
      "                     {'feature': 'Sin', 'score': 0.004231472600924544},\n",
      "                     {'feature': 'Bigelow', 'score': 0.004212174040417503}],\n",
      "     'prediction': 0.9994892924728337}}\n"
     ]
    }
   ],
   "source": [
    "# Generate explanations\n",
    "exp = explainer.explain_instance(\n",
    "    labels=[0, 1, 2], # which classes to produce explanations for?\n",
    "    instance=raw_test.data[9],\n",
    "    num_features=5 # how many words to show?\n",
    ")\n",
    "\n",
    "print('Label', raw_train.target_names[raw_test.target[9]], '=>', raw_test.target[9])\n",
    "pprint(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f6cbbf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: creps@lateran.ucs.indiana.edu (Stephen A. Creps)\\nSubject: Re: The doctrine of Original Sin\\nOrganization: Indiana University\\nLines: 63\\n\\nIn article <May.11.02.39.07.1993.28331@athos.rutgers.edu> Eugene.Bigelow@ebay.sun.com writes:\\n>>If babies are not supposed to be baptised then why doesn\\'t the Bible\\n>>ever say so.  It never comes right and says \"Only people that know\\n>>right from wrong or who are taught can be baptised.\"\\n>\\n>This is not a very sound argument for baptising babies. It assumes that\\n>if the Bible doesn\\'t say specifically that you don\\'t need to do something,\\n>then that must mean that you do need to do it. I know there\\'s a specific\\n>term for this form of logic, but it escapes me right now. However, if it\\n>were sound, then you should be able to apply it this way; If the Bible\\n>doesn\\'t specifically say that something is wrong, then it must be OK,\\n>which, coincidentally, leads perfectly into a question I\\'ve often pondered.\\n\\n   This is no less logical than the assumption that if something is\\n_not_ in the Bible, then it _must not_ be done.  But I don\\'t really\\nthink that\\'s what he\\'s saying anyway.  See below.\\n\\n>If slavery is immoral (which I believe it is, can I assume that everyone\\n>else in this group does too?), why doesn\\'t Jesus or any of the apostles\\n>speak out against it? Owning slaves was common practice back then. Paul\\n>speaks about everything else that is immoral. He apparently thought it\\n>was important enough to talk about things like not being a drunkard. Why\\n>doesn\\'t anyone mention slavery? If God\\'s morals are eternal and don\\'t\\n>change like the morals of society, then it must have been just as immoral then\\n>as it is today.\\n\\n   What about the letter to Philemon?  In it Paul at least hints that a\\ncertain slave be released.  Also, slavery in those times was not the\\nsame as the type of slavery we had in the U.S.  I think a better\\ncomparison would be to indentured servitude.  I don\\'t really want to get\\ninto a discussion on slavery.  Anyway, although it does demonstrate your\\npoint, I don\\'t think it is relevent, because the original poster did not\\nsay that absence of specific condemenation proves something is not\\nimmoral.\\n\\n   Back to the original poster\\'s assertion.  He is not in fact making\\nthe logical error of which you accuse him.  He stated the fact that the\\nBible does not say that babies cannot be baptized.  Also, we know that\\nthe Bible says that _everyone_ must be baptized to enter Heaven.\\n_Everyone_ includes infants, unless there is other Scripture to the\\ncontrary, i.e. an exception.  Since there is no exception listed in the\\nBible, we must assume (to be on the safe side) that the Bible means what\\nit says, that _everyone_ must be baptized to enter Heaven.  And so we\\nbaptize infants.\\n\\n   To summarize, you accused the original poster of saying if something\\nis not forbidden by the Bible, then that proves it is OK; i.e. if\\nsomething cannot be disproven, it is true.  He rather seemed to be\\nasserting that since the Bible does not forbid, _you cannot prove_,\\nusing the Bible, that it is _not_ OK.  There is a difference between\\nproving whether or not something can be proven or disproven (there are\\ntheories on provability in the field of Logic, by the way) and actually\\nproving or disproving it.  The other logical error we must avoid falling\\ninto is the converse: that if something cannot be proven, then it is\\nfalse.  This seems to be the error of many _sola scriptura_ believers.\\n\\n   I think the only thing that can be proven here is that one cannot use\\nScripture alone to prove something either way about infant Baptism,\\nalthough the evidence seems to me to favor it.\\n\\n-\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\nSteve Creps, Indiana University\\ncreps@lateran.ucs.indiana.edu\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb980332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 0.00044272540371258136,\n",
       " 'explanation': [{'feature': 'Sin', 'score': -0.0029887528528975776},\n",
       "  {'feature': 'Bigelow', 'score': -0.00298886560086397},\n",
       "  {'feature': 'Heaven', 'score': -0.0035761247900124604},\n",
       "  {'feature': 'Scripture', 'score': -0.003588574323814682},\n",
       "  {'feature': 'Bible', 'score': -0.007372212218436996}]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
